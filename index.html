<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ravi Shankar</title>
  
  <meta name="author" content="Ravi Shankar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ravi Shankar</name>
              </p>
              <p>I am a graduate student in the department of ECE at <a href="https://engineering.jhu.edu/ece/">Johns Hopkins University</a>, where I work on Speech signal processing.
              </p>
              <p>
                At JHU, I've primarily worked on emotion conversion in speech in the context of expressive speech synthesis. My work lies in the intersection of speech signal processing, statistical modeling, and deep learning. I am currently advised by <a href="https://scholar.google.com/citations?user=dDtlmCAAAAAJ&hl=en">Dr. Archana Venkataraman</a> who heads the <a href="https://engineering.jhu.edu/nsa/">NSA Lab</a> at JHU. I did my undergraduate in Electronics and Electrical Engineering at <a href="https://www.iitg.ac.in/">IIT Guwahati</a> where I worked on Keyword spotting for low-resourced languages supervised by <a href="https://scholar.google.co.in/citations?user=gakbTtAAAAAJ&hl=en">Dr. S.R.M Prasanna</a>. I've received the <a href="https://www.minds.jhu.edu/research/">MINDS fellowship award</a> for working on the frontiers of machine learning and data science. I have also been a recepient of <a href="https://www.daad.de/en/">DAAD-WISE</a> fellowship award in the past for doing research internship in Germany. 
              </p>
              <p style="text-align:center">
                <a href="mailto:ravishankar0841@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/cv_ravishankar.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=uGtWx6EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/MLE_Ravi">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ravi-0841">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ravi.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ravi.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in un/supervised learning, graphical modeling, and signal processing.
                My research is mainly about understanding and manipulating factors behind emotion perception in human speech.
                Here are the papers that I have published in the conferences/journals or are currently under review:
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/neural_net.jpg" type="image/jpg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/neural_net.jpg' width="180" height="120">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('nnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('nnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>A Deep-Bayesian Framework for Adaptive Speech Duration Modification</papertitle>
              <br>
              <strong>Ravi Shankar</strong>, 
              Archana Venkataraman
              <br>
              <em>Under Review</em>
              <br>
              <a href="https://github.com/ravi-0841/pytorch-speech-transformer">code</a>
              <p>We propose the first method to adaptively modify the duration of a given speech signal. Our approach uses a Bayesian framework to define a latent attention map that links frames of the input and target utterances.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vcgan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vcgan.jpg" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vcgan.jpg' width="180" height="120">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('vcgan_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('vcgan_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://isca-speech.org/archive/Interspeech_2020/abstracts/1325.html">
                <papertitle>Non-parallel Emotion Conversion using a Deep-Generative Hybrid Network and an Adversarial Pair Discriminator</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              Jacob Sager, Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2020
              <br>
              <a href="https://github.com/ravi-0841/variational-cycle-gan">code</a> /
              <a href="https://arxiv.org/abs/2007.12932">arXiv</a>
              <p></p>
              <p>Improved Cycle-GAN by using conditional density penalty in addition to cycle-consitency loss.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cedp_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/cedp.jpg" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/cedp.jpg' width="180" height="80">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('cedp_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('cedp_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://isca-speech.org/archive/Interspeech_2020/abstracts/1323.html">
                <papertitle>Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=oIySxE4AAAAJ">Hsi-Wei Hsieh</a>,
              <a href="https://scholar.google.com/citations?user=344v9UwAAAAJ&hl=en">Nicolas Charon</a>,
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2020
              <br>
              <a href="https://github.com/ravi-0841/Chained-Encoder-Decoder-Predictor">code</a> /
              <a href="https://arxiv.org/abs/2007.12937">arXiv</a>
              <p></p>
              <p>A chained model using latent variable regularization to mediate conversion from one emotion to another in speech.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sylseg_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/sylseg.png" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/sylseg.png' width="180" height="100">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('sylseg_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('sylseg_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/Interspeech_2019/abstracts/1450.html">
                <papertitle>Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>, 
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2019
              <br>
              <p></p>
              <p>We use the vowel/consonant peak identification in the loudness profile of speech to carry out syllable segmentation.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cedp_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/cedp.jpg" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/cedp.jpg' width="180" height="80">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('cedp_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('cedp_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://isca-speech.org/archive/Interspeech_2020/abstracts/1323.html">
                <papertitle>Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=oIySxE4AAAAJ">Hsi-Wei Hsieh</a>,
              <a href="https://scholar.google.com/citations?user=344v9UwAAAAJ&hl=en">Nicolas Charon</a>,
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2020
              <br>
              <a href="https://github.com/ravi-0841/Chained-Encoder-Decoder-Predictor">code</a> /
              <a href="https://arxiv.org/abs/2007.12937">arXiv</a>
              <p></p>
              <p>A chained model using latent variable regularization to mediate conversion from one emotion to another in speech.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/isca.gif" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, Interspeech 2021</strong>
              <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ciss.jpeg" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, CISS 2021</strong>
              <br><br>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website's template has been taken from here: <a href="https://jonbarron.info/">source code</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
