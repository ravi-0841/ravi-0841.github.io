<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-F1YHQV5BDJ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-F1YHQV5BDJ');
  </script>

  <title>Ravi Shankar</title>
  
  <meta name="author" content="Ravi Shankar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ravi Shankar</name>
              </p>
              <p>I am currently a Senior research engineer at Qualcomm in the Multimedia RnD team developing on-device models for speech processing. Prior to joining Qualcomm, I was a PhD candidate in the department of ECE at <a href="https://engineering.jhu.edu/ece/">Johns Hopkins University</a>, where I worked on expressive speech resynthesis.
              </p>
              <p>
                At JHU, I've primarily worked on generative modeling of prosody for expressive/emotional speech synthesis. My work lies in the intersection of speech signal processing, statistical modeling, and deep learning. I was advised by <a href="https://scholar.google.com/citations?user=dDtlmCAAAAAJ&hl=en">Dr. Archana Venkataraman</a> (head, <a href="https://engineering.jhu.edu/nsa/">NSA Lab</a>, JHU). I did my undergraduate in Electronics and Electrical Engineering at <a href="https://www.iitg.ac.in/">IIT Guwahati</a> where I worked on Keyword spotting for low-resourced languages supervised by <a href="https://scholar.google.co.in/citations?user=gakbTtAAAAAJ&hl=en">Dr. S.R.M Prasanna</a>. During the course of my PhD, I received my masters degree in Applied Math and Statistics at JHU. I've been the receipient of <a href="https://engineering.jhu.edu/ece/2021/02/15/three-ece-students-named-minds-fellows-for-spring-semester/">MINDS fellowship award</a> twice for working on the frontiers of machine learning and data science. I have also received ECE graduate fellowship at JHU and <a href="https://www.daad.de/en/">DAAD-WISE</a> fellowship award in the past for doing research internship in Germany. 
              </p>
              <p style="text-align:center">
                <a href="mailto:ravishankar0841@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/cv_RaviShankar_updated.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=uGtWx6EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/MLE_Ravi">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ravi-0841">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ravi.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ravi.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in un/supervised learning, graphical modeling, and signal processing.
                My research is mainly about understanding and manipulating prosodic information to alter emotion perception in human speech.
                Here are the papers that I have published in the conferences/journals or are currently under review:
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/prosodic_features-1.png" type="image/jpg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/prosodic_features-1.png' width="150" height="80">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('nnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('nnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Manipulating Emotions: Generative Modeling of Prosody for Emotional Speech Synthesis </papertitle>
              <br>
              <strong>Ravi Shankar</strong>
              <br>
              <em>Johns Hopkins University</em>
              <br>
              <a href="data/Johns_Hopkins_University_Thesis.pdf">Thesis</a>
              <p>In this thesis, we devise different ways of learning prosody modeling techniques to inject emotion into neutral speech. We develop supervised algorithms for F0, energy and rhythm modification followed by unsupervised approaches that combines probabilistic graphical modeling with neural network as density function.</p>
            </td>
          </tr>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/GCRN_model_extended_2-1.png" type="image/jpg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/GCRN_model_extended_2-1.png' width="150" height="80">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('nnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('nnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>A Closer Look at Wav2Vec2 Embeddings for On-device Single-channel Speech Enhancement</papertitle>
              <br>
              <strong>Ravi Shankar</strong>, 
              Ke Tan,
              Buye Xu,
              Anurag Kumar
              <br>
              <em>ICASSP 2024</em>
              <br>
              <a href="data/SC-SE.pdf">Paper</a>
              <p>In this work, we proposed different ways of using Wav2Vec2 embeddings for single channel speech enhancement. Our study shows that in a constrained (low-memory/poor SNR/causality) settings, SSL embeddings fail to provide helpful information to improve enhancement task.</p>
            </td>
          </tr>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/neural_net.jpg" type="image/jpg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/neural_net.jpg' width="180" height="120">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('nnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('nnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Adaptive Duration Modification of Speech using Masked Convolutional Networks and Open-Loop Time Warping</papertitle>
              <br>
              <strong>Ravi Shankar</strong>, 
              Archana Venkataraman
              <br>
              <em>ISCA SSW12, 2023</em>
              <br>
              <a href="https://github.com/ravi-0841/pytorch-speech-transformer">code</a> / 
              <a href="https://www.isca-speech.org/archive/ssw_2023/shankar23_ssw.html">Paper</a>
              <p>We propose the first method to adaptively modify the duration of a given speech signal. Our approach uses a Bayesian framework to define a latent attention map that links frames of the input and target utterances.</p>
            </td>
          </tr>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/diff_demo.jpg" type="image/jpg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/diff_demo.jpg' width="150" height="80">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('nnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('nnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9903577">
              <papertitle>A Diffeomorphic Flow-based Variational Framework for Multi-speaker Emotion Conversion</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>, 
              <a href="https://scholar.google.com/citations?hl=en&user=oIySxE4AAAAJ">Hsi-Wei Hsieh</a>,
              <a href="https://scholar.google.com/citations?user=344v9UwAAAAJ&hl=en">Nicolas Charon</a>,
              Archana Venkataraman
              <br>
              <em>IEEE/ACM Transactions on Audio, Speech and Language Processing, 2022</em>
              <br>
              <a href="https://github.com/ravi-0841/spect-pitch-gan">code</a> / 
              <a href="https://arxiv.org/abs/2211.05071">arXiv</a>
              <p>In this work, we extensively study the cycle consistency loss in the context of Cycle-GAN model. We identify some of its major shortcomings and propose a new loss function to address the pitfalls.</p>
            </td>
          </tr>

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/recognition_models.png" type="image/jpg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/recognition_models.png' width="150" height="80">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('nnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('nnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>A Comparative Study of Data Augmentation Techniques for Deep Learning Based Emotion Recognition</papertitle>
              <br>
              <strong>Ravi Shankar</strong>, 
              Abdouh Harouna,
              Arjun Somayazulu,
              Archana Venkataraman
              <br>
              <a href="https://arxiv.org/abs/2211.05047">arXiv</a>
              <p>We comprehensively study different types of data augmentation procedures in the context of speech emotion recognition. Our study spans multiple neural architectures and datasets for an unbiased comparison.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vcgan_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vcgan.jpg" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vcgan.jpg' width="180" height="120">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('vcgan_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('vcgan_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2020/shankar20c_interspeech.html">
                <papertitle>Non-parallel Emotion Conversion using a Deep-Generative Hybrid Network and an Adversarial Pair Discriminator</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              Jacob Sager, Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2020
              <br>
              <a href="https://github.com/ravi-0841/variational-cycle-gan">code</a> /
              <a href="https://arxiv.org/abs/2007.12932">arXiv</a>
              <p></p>
              <p>Improved Cycle-GAN by using KL divergence penalty on the conditional density in addition to cycle-consitency loss.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cedp_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/cedp.jpg" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/cedp.jpg' width="180" height="80">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('cedp_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('cedp_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2020/shankar20b_interspeech.html">
                <papertitle>Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=oIySxE4AAAAJ">Hsi-Wei Hsieh</a>,
              <a href="https://scholar.google.com/citations?user=344v9UwAAAAJ&hl=en">Nicolas Charon</a>,
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2020
              <br>
              <a href="https://github.com/ravi-0841/Chained-Encoder-Decoder-Predictor">code</a> /
              <a href="https://arxiv.org/abs/2007.12937">arXiv</a>
              <p></p>
              <p>A chained model using latent variable regularization to mediate conversion from one emotion to another in speech.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sylseg_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/sylseg.png" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/sylseg.png' width="180" height="100">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('sylseg_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('sylseg_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2019/shankar19_interspeech.html">
                <papertitle>Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>, 
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2019
              <br>
              <p></p>
              <p>We use the vowel/consonant peak identification in the loudness profile of speech to carry out syllable segmentation.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vesus_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vesus.png" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vesus.png' width="180" height="100">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('vesus_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('vesus_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2019/sager19_interspeech.html">
                <papertitle>VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English</papertitle>
              </a>
              <br>
              Jacob Sager, 
              <strong>Ravi Shankar</strong>, 
              <a href="https://scholar.google.com/citations?user=WfeOXZgAAAAJ&hl=en">Jacob Reinhold</a>, 
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2019 (<strong>Oral</strong>)
              <br>
              <a href="https://engineering.jhu.edu/nsa/vesus/">dataset</a>
              <p></p>
              <p>VESUS corpus contains 250 phrases spoken by 10 different actors in 5 emotion categories. The objective is to study factors underlying emotion perception in a lexically controlled environment.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hnet.png" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/hnet.png' width="180" height="100">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('hnet_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('hnet_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2019/shankar19b_interspeech.html">
                <papertitle>A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>, 
              Jacob Sager, 
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2019 (<strong>Oral</strong>)
              <br>
              <a href="https://github.com/ravi-0841/Emocon">code</a>
              <p></p>
              <p>We propose a perturbation model for F0 and energy prediction using highway network. The model is trained to maximize the likelihood of error in an EM framework.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='momenta_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/momenta_hnet.png" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/momenta_hnet.png' width="180" height="100">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('momenta_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('momenta_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2019/shankar19c_interspeech.html">
                <papertitle>Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=oIySxE4AAAAJ">Hsi-Wei Hsieh</a>,
              <a href="https://scholar.google.com/citations?user=344v9UwAAAAJ&hl=en">Nicolas Charon</a>,
              Archana Venkataraman
              <br>
              <em>Interspeech</em>, 2019
              <br>
              <a href="https://plmlab.math.cnrs.fr/benjamin.charlier/fshapesTk">code</a>
              <p></p>
              <p>We use diffeomorphic registration to model the target emotion F0 contour. It serves as a regularization technique for better target F0 range approximation.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='keyword_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/spoken_keyword.png" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/spoken_keyword.png' width="180" height="100">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('keyword_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('keyword_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive/interspeech_2018/shankar18_interspeech.html">
                <papertitle>Spoken Keyword Detection Using Joint DTW-CNN</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              Vikram C.M., 
              <a href="https://scholar.google.co.in/citations?user=gakbTtAAAAAJ&hl=en">S.R.M Prasanna</a>
              <br>
              <em>Interspeech</em>, 2018 (<strong>Oral</strong>)
              <br>
              <p></p>
              <p>In this paper, we propose a randomized DTW method coupled with convolutional network to identify presence/absence of a keyword.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='morphological_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/morphological_keyword.png" type="image/jpeg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/morphological_keyword.png' width="180" height="100">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('morphological_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('morphological_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7561151">
                <papertitle>Spoken term detection from continuous speech using ANN posteriors and image processing techniques</papertitle>
              </a>
              <br>
              <strong>Ravi Shankar</strong>,
              Arpit Jain, Deepak K.T., Vikram C.M., 
              <a href="https://scholar.google.co.in/citations?user=gakbTtAAAAAJ&hl=en">S.R.M Prasanna</a>
              <br>
              <em>NCC</em>, 2016
              <br>
              <p></p>
              <p>We propose a sequence of morphological operation to refine the DTW matrix for easier keyword spotting.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/JHU_WSE_logo.jpeg" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Teaching Assistant, Probabilistic Machine Learning (EN.520.651) Fall 2021, 2022</strong>
              <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/icml.png" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, ICML 2024</strong>
              <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/neurips.jpg" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, NeuRips 2022, 2023</strong>
              <br><br>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/uailogo_square.png" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, UAI 2023</strong>
              <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/iclr.svg" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, ICLR 2022, 2024</strong>
              <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/isca.gif" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, Interspeech 2021, 2022, 2023, 2024</strong>
              <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ciss.jpeg" width="100" height="80"></td>
            <td width="75%" valign="center">
              <br>
              <strong>Reviewer, CISS 2021</strong>
              <br><br>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website's template has been taken from here: <a href="https://github.com/jonbarron/website">source code</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
